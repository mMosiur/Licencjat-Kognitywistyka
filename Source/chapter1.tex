\chapter{Neuronaukowe korzenie uczenia maszynowego}

Już niedługo po pierwszych teoriach i propozycjach dotyczących maszyn przeprowadzających obliczenia przedstawiły się one jako ciekawa koncepcja dla filozofii umysłu i psychologii.
Maszyna Turinga ze swoimi możliwościami przetwarzania informacji oraz terminologia z nią związana stały się nową popularną metaforą działania umysłu.
Aktualnie prawie każdy większy dział nauk humanistycznych ma swój komputacjonistyczny odłam skupiający się na tym właśnie aspekcie jako wbudowanej cesze świata rzeczywistego.
Był to kolejny z wielu innych trendów analogii i metafory w psychologii \cite{vroon1987man}, jednak z biegiem czasu przez wielu uważany za najbliższy prawdy.

Największą różnicą między możliwościami umysłu człowieka a tym, co w przyszłości zostanie nazwane komputerem była koncepcja uczenia się.
Pierwsze użycie sformułowania "uczenie maszynowe" przypisuje się Arturowi Samuelowi, który takim określeniem opisał algorytmy podejmujące decyzje bez wcześniejszego jawnego zaprogramowania ich do takiego ich wykonywania \cite{koza1996automated}.
Dzisiaj dział informatyki, nazwany identycznie jak wspomniana grupa algorytmów, skupia w sobie wiele różnych paradygmatów i podejść, z których każde ma własne założenia, ograniczenia i osiągnięcia.
Jednymi z najwcześniej używanych były populacyjno-stochastyczne algorytmy genetyczne, inspirowane ewolucyjnym charakterem przyrody.

Tak jak psychologia czerpała z informatyki inspirację przy budowaniu metafory komputerowej, tak dziedzina uczenia maszynowego inspirowała się obecnymi w naturze zjawiskami wszelkiego rodzaju uczenia się.
Idea algorytmów genetycznych i programowania genetycznego została wyprowadzona jako bezpośrednie przeniesienie teorii ewolucji w naturze.
Jest to jednocześnie przykład, jak uczenie maszynowe może poprzez mechanizm sprzężenia zwrotnego wspomóc obszar jego pierwotnej inspiracji.
Użycie algorytmów genetycznych było pomocne przy demonstracjach i dowodzeniu teorii z zakresu Darwinowskiej ewolucji, takich jak wpływ adaptacji na selekcję, i zwrotny wpływ selekcji na adaptację \cite{bruderer1996organizational}.

Najbardziej fundamentalna jednostka popularnych dzisiaj sztucznych sieci neuronowych -- sztuczny neuron, czy perceptron -- zostały po raz pierwszy zaproponowane jako matematyczny model naśladujący działanie neuronu biologicznego \cite{mcculloch1943logical}.
Początkowo neuron McCullocha-Pittsa bezpośrednio odwzorowywał działanie biologicznego odpowiednika, w tym na przykład traktując wyjście modelu binarnie jako \emph{aktywne} lub \emph{nieaktywne}.
Później także podobnie jak w biologicznym układzie nerwowym złożonym ze skomplikowanej sieci neuronów zaczęto wykorzystywać perceptrony wielowarstwowe propagujące sygnał wgłąb modelu \cite{rosenblatt1961principles}.
Obszar uczenia maszynowego skupiający się właśnie na wykorzystaniu takich wielowarstwowych sztucznych sieci neuronowych został nazwany \emph{uczeniem głębokim}, podkreślając jednocześnie rolę ilości warstw w skuteczności działania takich sieci.

Dodatkowo od strony neuronauki pokładano nadzieje, że rozwój tych sztucznych systemów wzorowanych na biologicznym układzie nerwowym pozwoli nam w lepszy sposób zrozumieć działanie mózgu, podobnie jak w przypadku algorytmów genetycznych.
Znacznie prostsze jest zrozumienie złożonego systemu poprzez zbudowanie go od podstaw niż próby wykorzystania inżynierii wstecznej na danych obserwacyjnych z nim związanych \cite{braitenberg1986vehicles}.

Obie dziedziny zaczęły się jednak od siebie coraz bardziej oddalać.

Rozwój uczenia maszynowego -- w tym przede wszystkim uczenia głębokiego -- opierał się na optymalizacji funkcji kosztu i odkryciach natury matematycznej z tym związanych \cite{sutskever2013importance}.
Mimo tego, że koncepcje podstawowe wykorzystane w sztucznych sieciach neuronowych były bezpośrednio zainspirowane biologicznymi odpowiednikami, szybko okazało się, że wprowadzane modyfikacje pierwotnej architektury i działania sieci znacznie poprawiają ich skuteczność.
Zmiany te jednak nie tylko nie były wynikiem przełożenia wiedzy neuronaukowej do uczenia maszynowego, a wręcz przeciwnie -- odstępowały od podobieństwa do natury na rzecz złożonych modeli matematycznych oraz udogodnień i usprawnień z nich wynikających.
Problem funkcji kosztu okazał się wyjątkowo ważny w sieciach głębokich, a do jego rozwiązania zaczęto wykorzystywać algorytm wstecznej propagacji błędu \cite{rumelhart1985learning}.
Jego skuteczność zależała natomiast od wykorzystanej funkcji aktywacji.
Funkcja binarna funkcja -- podobna do zachowania neuronu biologicznego -- okazała się gorsza od funkcji liniowych \cite{minsky2017perceptrons}, a te znowu ustępowały funkcjom nieliniowym \cite{haykin1994neural}.
Nowe wykorzystywane funkcje były skuteczniejsze i efektywniejsze \cite{sharma2017activation} w rozwiązywaniu problemów poszukiwania minimum funkcji kosztu.
Jednocześnie oddalały się od swoich biologicznych korzeni działania.

Z drugiej strony w neuronauce odkryto wiele typów komórek, stanów komórkowych, mechanizmów obliczeniowych i przechowywania informacji oraz obszarów mózgowych \cite{solari2011cognitive}.
Nacisk kładziony był raczej na strukturę i budowę mózgu oraz różnej skali elementów go budujących, a sieci neuronowe zaczęły przyjmować raczej rolę zwykłego narzędzia jak w każdej innej dziedzinie, bez większych, mocniejszych czy bezpośrednich powiązań.
Szukano w mózgu podobnych technik działania jak te implementowane w sztucznych sieciach neuronowych, jednak okazywały się one w zdecydowanej większości nieistniejące.
Wcześniej wspomniana wsteczna propagacja błędu nie ma swojego odpowiednika w mózgu \cite{hassabis2017neuroscience}, nawet takiego, który byłby tylko zbliżony ideowo.

Warto również wspomnieć, że rozwój obszaru uczenia maszynowego nie wpłynął na dziedziny odnoszące się do komputerów jako systemów symbolicznych i ich zestawienia z człowiekiem.
W psychologii nadal dominowała i dominuje metafora komputerowa \cite{casey1989computational}, nie przeniesiono uwagi na sztuczne sieci neuronowe, mimo, że wydają się one z pozoru być lepszym odzwierciedleniem mózgu człowieka, głównego źródła poznania i umysłu.
Podobnie w filozofii -- nawet, gdy sieci neuronowe były już znane, nadal odwoływano się do komputera jako całości sprowadzając uczenie maszynowe do wyłącznie algorytmu wykonywanego przez urządzenie symboliczne \cite{searle1980minds}.
Ogólne postrzeganie tej dziedziny sztucznej inteligencji nie wydawało się więc być bliżej działania mózgu -- a w konsekwencji człowieka -- niż tradycyjny pogląd na komputer.
